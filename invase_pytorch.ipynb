{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VbkN_QB6G5BX",
        "ZlXrb99jHXHD"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPkoanOKsY1l/IM32/uKEmM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/TBeatrix/43ca56bac4956bb960a39df1cdc3d5d1/invase_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INVASE algorithm implementation in Pytorch**"
      ],
      "metadata": {
        "id": "r_fqVL-Y7w7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on\n",
        "\n",
        "Reference: Jinsung Yoon, James Jordon, Mihaela van der Schaar, \n",
        "           \"IINVASE: Instance-wise Variable Selection using Neural Networks,\" \n",
        "\n",
        "           International Conference on Learning Representations (ICLR), 2019.\n",
        "\n",
        "Paper Link: https://openreview.net/forum?id=BJg_roAcK7\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "FjYR_N1B74zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Necessary packages\n",
        "import numpy as np \n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function"
      ],
      "metadata": {
        "id": "TOVgxYJbHMHd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data generation**\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "VbkN_QB6G5BX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sHEpVoEY7hQX"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Generating 6 synthetic datasets\n",
        "x ~ N(0,I) where d = 11 or 100\n",
        "y = 1/(1+logit) where logit for each synthetic dataset is\n",
        "- syn1: logit = exp(x1 * x2)\n",
        "- syn2: logit = exp(x3^2 + x4^2 + x5^2 + x6^2 -4)\n",
        "- syn3: logit = -10 sin(2 * x7) + 2|x8| + x9 + exp(-x10) - 2.4\n",
        "- syn4: If x11 < 0, follows syn1, else if x11 >= 0, follows syn2\n",
        "- syn5: If x11 < 0, follows syn1, else if x11 >= 0, follows syn3\n",
        "- syn6: If x11 < 0, follows syn2, else if x11 >= 0, follows syn3\n",
        "\"\"\"\n",
        "\n",
        "def generate_x (n, dim):\n",
        "  \"\"\"Generate the features (x).\n",
        "  \n",
        "  Args:\n",
        "    - n: the number of samples\n",
        "    - dim: the number of features (feature dimensions)\n",
        "    \n",
        "  Returns:\n",
        "    - x: (n x dim) data points sample from N(0, 1)\n",
        "  \"\"\"\n",
        "  x = np.random.randn(n, dim)\n",
        "  return x\n",
        "\n",
        "def generate_y (x, data_type):\n",
        "  \"\"\"Generate corresponding label (y) given feature (x).\n",
        "  \n",
        "  Args:\n",
        "    - x: features\n",
        "    - data_type: synthetic data type (syn1 to syn6)\n",
        "\n",
        "  Returns:\n",
        "    - y: corresponding labels\n",
        "  \"\"\"\n",
        "  # number of samples\n",
        "  n = x.shape[0]\n",
        "    \n",
        "  # Logit computation\n",
        "  if data_type == 'syn1':\n",
        "    logit = np.exp(x[:, 0]*x[:, 1])\n",
        "  elif data_type == 'syn2':       \n",
        "    logit = np.exp(np.sum(x[:, 2:6]**2, axis = 1) - 4.0) \n",
        "  elif data_type == 'syn3':\n",
        "    logit = np.exp(-10 * np.sin(0.2*x[:, 6]) + abs(x[:, 7]) + \\\n",
        "                   x[:, 8] + np.exp(-x[:, 9])  - 2.4)     \n",
        "  elif data_type == 'syn4':\n",
        "    logit1 = np.exp(x[:, 0]*x[:, 1])\n",
        "    logit2 = np.exp(np.sum(x[:, 2:6]**2, axis = 1) - 4.0) \n",
        "  elif data_type == 'syn5':\n",
        "    logit1 = np.exp(x[:, 0]*x[:, 1])\n",
        "    logit2 = np.exp(-10 * np.sin(0.2*x[:, 6]) + abs(x[:, 7]) + \\\n",
        "                    x[:, 8] + np.exp(-x[:, 9]) - 2.4) \n",
        "  elif data_type == 'syn6':\n",
        "    logit1 = np.exp(np.sum(x[:,2:6]**2, axis = 1) - 4.0) \n",
        "    logit2 = np.exp(-10 * np.sin(0.2*x[:, 6]) + abs(x[:, 7]) + \\\n",
        "                    x[:, 8] + np.exp(-x[:, 9]) - 2.4) \n",
        "    \n",
        "  # For syn4, syn5 and syn6 only\n",
        "  if data_type in ['syn4', 'syn5', 'syn6']:\n",
        "    # Based on X[:,10], combine two logits        \n",
        "    idx1 = (x[:, 10]< 0)*1\n",
        "    idx2 = (x[:, 10]>=0)*1    \n",
        "    logit = logit1 * idx1 + logit2 * idx2    \n",
        "        \n",
        "  # Compute P(Y=0|X)\n",
        "  prob_0 = np.reshape((logit / (1+logit)), [n, 1])\n",
        "    \n",
        "  # Sampling process\n",
        "  y = np.zeros([n, 2])\n",
        "  y[:, 0] = np.reshape(np.random.binomial(1, prob_0), [n,])\n",
        "  y[:, 1] = 1-y[:, 0]\n",
        "\n",
        "  return y\n",
        "\n",
        "\n",
        "def generate_ground_truth(x, data_type):\n",
        "  \"\"\"Generate ground truth feature importance corresponding to the data type\n",
        "     and feature.\n",
        "  \n",
        "  Args:\n",
        "    - x: features\n",
        "    - data_type: synthetic data type (syn1 to syn6)\n",
        "\n",
        "  Returns:\n",
        "    - ground_truth: corresponding ground truth feature importance\n",
        "  \"\"\"\n",
        "\n",
        "  # Number of samples and features\n",
        "  n, d = x.shape\n",
        "\n",
        "  # Output initialization\n",
        "  ground_truth = np.zeros([n, d])\n",
        "        \n",
        "  # For each data_type\n",
        "  if data_type == 'syn1':\n",
        "    ground_truth[:, :2] = 1\n",
        "  elif data_type == 'syn2':\n",
        "    ground_truth[:, 2:6] = 1\n",
        "  elif data_type == 'syn3':\n",
        "    ground_truth[:, 6:10] = 1\n",
        "        \n",
        "  # Index for syn4, syn5 and syn6\n",
        "  if data_type in ['syn4', 'syn5', 'syn6']:        \n",
        "    idx1 = np.where(x[:, 10]<0)[0]\n",
        "    idx2 = np.where(x[:, 10]>=0)[0]\n",
        "    ground_truth[:, 10] = 1\n",
        "        \n",
        "  if data_type == 'syn4':        \n",
        "    ground_truth[idx1, :2] = 1\n",
        "    ground_truth[idx2, 2:6] = 1\n",
        "  elif data_type == 'syn5':        \n",
        "    ground_truth[idx1, :2] = 1\n",
        "    ground_truth[idx2, 6:10] = 1\n",
        "  elif data_type == 'syn6':        \n",
        "    ground_truth[idx1, 2:6] = 1\n",
        "    ground_truth[idx2, 6:10] = 1\n",
        "        \n",
        "  return ground_truth\n",
        "    \n",
        "def generate_dataset(n = 10000, dim = 11, data_type = 'syn1', seed = 0):\n",
        "  \"\"\"Generate dataset (x, y, ground_truth).\n",
        "  \n",
        "  Args:\n",
        "    - n: the number of samples\n",
        "    - dim: the number of dimensions\n",
        "    - data_type: synthetic data type (syn1 to syn6)\n",
        "    - seed: random seed\n",
        "    \n",
        "  Returns:\n",
        "    - x: features\n",
        "    - y: labels\n",
        "    - ground_truth: ground truth feature importance\n",
        "  \"\"\"\n",
        "\n",
        "  # Seed\n",
        "  np.random.seed(seed)\n",
        "\n",
        "  # x generation\n",
        "  x = generate_x(n, dim)\n",
        "  # y generation\n",
        "  y = generate_y(x, data_type)\n",
        "  # ground truth generation\n",
        "  ground_truth = generate_ground_truth(x, data_type)\n",
        "  \n",
        "  return x, y, ground_truth"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Utility functions**\n",
        "(1) Feature performance metrics\n",
        "\n",
        "(2) Prediction performance metrics\n",
        "\n",
        "(3) Bernoulli sampling\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ZlXrb99jHXHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_performance_metric (ground_truth, importance_score):\n",
        "  \"\"\"Performance metrics for feature importance (TPR and FDR).\n",
        "  Args:\n",
        "    - ground_truth: ground truth feature importance\n",
        "    - importance_score: computed importance scores for each feature\n",
        "    \n",
        "  Returns:\n",
        "    - mean_tpr: mean value of true positive rate\n",
        "    - std_tpr: standard deviation of true positive rate\n",
        "    - mean_fdr: mean value of false discovery rate\n",
        "    - std_fdr: standard deviation of false discovery rate\n",
        "  \"\"\"\n",
        "\n",
        "  n = importance_score.shape[0]\n",
        "  \n",
        "  tpr = np.zeros([n, ])\n",
        "  fdr = np.zeros([n, ])\n",
        "\n",
        "  # For each sample\n",
        "  for i in range(n):    \n",
        "    # tpr   \n",
        "    tpr_nom = np.sum(importance_score[i, :] * ground_truth[i, :])\n",
        "    tpr_den = np.sum(ground_truth[i, :])\n",
        "    tpr[i] = 100 * float(tpr_nom)/float(tpr_den + 1e-8)\n",
        "        \n",
        "    # fdr\n",
        "    fdr_nom = np.sum(importance_score[i, :] * (1-ground_truth[i, :]))\n",
        "    fdr_den = np.sum(importance_score[i,:])\n",
        "    fdr[i] = 100 * float(fdr_nom)/float(fdr_den+1e-8)\n",
        "    \n",
        "  mean_tpr = np.mean(tpr)\n",
        "  std_tpr = np.std(tpr)\n",
        "  mean_fdr = np.mean(fdr)\n",
        "  std_fdr = np.std(fdr)  \n",
        "  \n",
        "  return mean_tpr, std_tpr, mean_fdr, std_fdr"
      ],
      "metadata": {
        "id": "h5K_yKE_ZDEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_performance_metric (y_test, y_hat):\n",
        "  \"\"\"Performance metrics for prediction (AUC, APR, Accuracy).\n",
        "  \n",
        "  Args:\n",
        "    - y_test: testing set labels\n",
        "    - y_hat: prediction on testing set\n",
        "    \n",
        "  Returns:\n",
        "    - auc: area under roc curve\n",
        "    - apr: average precision score\n",
        "    - acc: accuracy\n",
        "  \"\"\"\n",
        "  \n",
        "  auc = roc_auc_score (y_test[:, 1], y_hat[:, 1])\n",
        "  apr = average_precision_score (y_test[:, 1], y_hat[:, 1])\n",
        "  acc = accuracy_score (y_test[:, 1], 1.*(y_hat[:, 1] > 0.5))\n",
        "  \n",
        "  return auc, apr, acc"
      ],
      "metadata": {
        "id": "WcGY9mQkHs3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bernoulli_sampling (prob):\n",
        "  \"\"\" Sampling Bernoulli distribution by given probability.\n",
        "  \n",
        "  Args:\n",
        "    - prob: P(Y = 1) in Bernoulli distribution.\n",
        "    \n",
        "  Returns:\n",
        "    - samples: samples from Bernoulli distribution\n",
        "  \"\"\"  \n",
        "\n",
        "  n, d = prob.shape\n",
        "  samples = np.random.binomial(1, prob, (n, d))\n",
        "        \n",
        "  return samples"
      ],
      "metadata": {
        "id": "-nUqmKszHw7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INVASE Model**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "1sG8EIWAH0Wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "ZxcCqny5__H-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.modules import activation\n",
        "class Invase():\n",
        "\n",
        "    def __init__(self, x_train, y_train, model_type, model_parameters):\n",
        "        \n",
        "        self.lamda = model_parameters['lamda']\n",
        "        self.actor_h_dim = model_parameters['actor_h_dim']\n",
        "        self.critic_h_dim = model_parameters['critic_h_dim']\n",
        "        self.n_layer = model_parameters['n_layer']\n",
        "        self.batch_size = model_parameters['batch_size']\n",
        "        self.iteration = model_parameters['iteration']\n",
        "        if model_parameters['activation'] == \"relu\":\n",
        "          self.activation = nn.ReLU()\n",
        "        self.learning_rate = model_parameters['learning_rate']\n",
        "        self.dim = x_train.shape[1]     \n",
        "        self.label_dim = y_train.shape[1]\n",
        "        self.model_type = model_type\n",
        "\n",
        "        # Build and compile critic\n",
        "        self.critic = self.build_critic()\n",
        "        self.critic.optimizer = torch.optim.Adam(self.critic.parameters(), self.learning_rate, weight_decay=1e-3)\n",
        "        self.critic.loss = nn.CrossEntropyLoss()\n",
        "       \n",
        "        # Build and compile the actor\n",
        "        self.actor = self.build_actor()\n",
        "        self.actor.optimizer = torch.optim.Adam(self.actor.parameters(), self.learning_rate, weight_decay=1e-3) \n",
        "        self.actor.loss = self.actor_loss\n",
        "\n",
        "        if self.model_type == 'invase':\n",
        "            # Build and compile the baseline\n",
        "            self.baseline = self.build_baseline()\n",
        "            self.baseline.optimizer = torch.optim.Adam(self.baseline.parameters(), self.learning_rate, weight_decay=1e-3)\n",
        "            self.baseline.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    def actor_loss(self, y_true, y_pred):\n",
        "        '''\n",
        "        y_true contains all outputs (actor, critic, basline) + ground truth\n",
        "        '''\n",
        "        # Actor output\n",
        "        actor_out = y_true[:, :self.dim]\n",
        "        # Critic output\n",
        "        critic_out = y_true[:, self.dim:(self.dim+self.label_dim)]\n",
        "\n",
        "        if self.model_type == 'invase':\n",
        "            # Baseline output\n",
        "            baseline_out = y_true[:, (self.dim+self.label_dim):(self.dim+2*self.label_dim)]\n",
        "            # Ground truth label\n",
        "            y_ground_truth = y_true[:, (self.dim+2*self.label_dim):]        \n",
        "        elif self.model_type == 'invase_minus':\n",
        "            # Ground truth label\n",
        "            y_ground_truth = y_true[:, (self.dim+self.label_dim):]         \n",
        "\n",
        "        # Critic loss\n",
        "        critic_loss = -torch.sum(y_ground_truth * torch.log(critic_out + 1e-8), dim=1)  \n",
        "        \n",
        "        if self.model_type == 'invase':        \n",
        "            # Baseline loss\n",
        "            baseline_loss = -torch.sum(y_ground_truth * torch.log(baseline_out + 1e-8), dim=1)  \n",
        "            # Reward\n",
        "            Reward = -(critic_loss - baseline_loss)\n",
        "\n",
        "        elif self.model_type == 'invase_minus':\n",
        "            Reward = -critic_loss\n",
        "\n",
        "        # Policy gradient loss computation. \n",
        "          \n",
        "        custom_actor_loss = Reward * torch.sum(actor_out * torch.log(y_pred + 1e-8) + \\\n",
        "                                (1-actor_out)* torch.log(1-y_pred + 1e-8), dim=1) - \\\n",
        "                                self.lamda * torch.mean(y_pred, dim=1)\n",
        "\n",
        "        # Custom actor loss\n",
        "        custom_actor_loss = torch.mean(-custom_actor_loss)\n",
        "\n",
        "        return custom_actor_loss\n",
        "\n",
        "    def build_actor(self):\n",
        "\n",
        "        class Actor(nn.Module):\n",
        "          def __init__(self, params):\n",
        "            super(Actor, self).__init__()\n",
        "            # Params\n",
        "            self.activation = params.activation\n",
        "            self.n_layer = params.n_layer\n",
        "            self.dim = params.dim\n",
        "            self.actor_h_dim = params.actor_h_dim\n",
        "\n",
        "            # Layers\n",
        "            self.linear_in = nn.Linear(self.dim, self.actor_h_dim)\n",
        "            self.linear_hidden = nn.Linear(self.actor_h_dim, self.actor_h_dim)\n",
        "            self.linear_out = nn.Linear(self.actor_h_dim, self.dim)\n",
        "\n",
        "          def forward(self, feature):\n",
        "             x = self.activation(self.linear_in(feature))\n",
        "             for i in range(self.n_layer -2):\n",
        "               x = self.activation(self.linear_hidden(x))\n",
        "             selection_probability = torch.sigmoid(self.linear_out(x))\n",
        "             return selection_probability\n",
        "\n",
        "        actor_model = Actor(self)\n",
        "        return actor_model\n",
        "\n",
        "    def build_critic(self):\n",
        "\n",
        "        class Critic(nn.Module):\n",
        "          def __init__(self, params):\n",
        "            super(Critic, self).__init__()\n",
        "            # Params\n",
        "            self.activation = params.activation\n",
        "            self.n_layer = params.n_layer\n",
        "            self.label_dim = params.label_dim\n",
        "            self.dim1 = params.dim\n",
        "            self.critic_h_dim = params.critic_h_dim\n",
        "\n",
        "            # Layers\n",
        "            self.linear_in = nn.Linear(self.dim1, self.critic_h_dim)\n",
        "            self.linear_hidden = nn.Linear(self.critic_h_dim, self.critic_h_dim)\n",
        "            self.linear_out = nn.Linear(self.critic_h_dim, self.label_dim)\n",
        "            self.batch_normalization = nn.BatchNorm1d(self.critic_h_dim)\n",
        "\n",
        "          def forward(self, feature, selection):\n",
        "               # Element wise multiplication\n",
        "             \n",
        "               critic_model_input = (feature * selection).float()\n",
        "             \n",
        "               x = self.activation(self.linear_in(critic_model_input))\n",
        "               x = self.batch_normalization(x)\n",
        "               for i in range(self.n_layer -2):\n",
        "                  x = self.activation(self.linear_hidden(x))\n",
        "                  x = self.batch_normalization(x)\n",
        "               y_hat = nn.Softmax(dim=1)(self.linear_out(x))\n",
        "               return y_hat\n",
        "        critic_model = Critic(self)\n",
        "        return critic_model\n",
        "\n",
        "\n",
        "    def build_baseline(self):\n",
        "\n",
        "         class Baseline(nn.Module):\n",
        "          def __init__(self, params):\n",
        "            super(Baseline, self).__init__()\n",
        "            # Params\n",
        "            self.activation = params.activation\n",
        "            self.n_layer = params.n_layer\n",
        "            self.label_dim = params.label_dim\n",
        "            self.dim = params.dim\n",
        "            self.baseline_h_dim = params.critic_h_dim # same as the critic\n",
        "\n",
        "            # Layers\n",
        "            self.linear_in = nn.Linear(self.dim, self.baseline_h_dim)\n",
        "            self.linear_hidden = nn.Linear(self.baseline_h_dim, self.baseline_h_dim)\n",
        "            self.linear_out = nn.Linear(self.baseline_h_dim, self.label_dim)\n",
        "            self.batch_normalization = nn.BatchNorm1d(self.baseline_h_dim)\n",
        "\n",
        "          def forward(self, feature):\n",
        "               x = self.activation(self.linear_in(feature))\n",
        "               x = self.batch_normalization(x)\n",
        "               for i in range(self.n_layer -2):\n",
        "                  x = self.activation(self.linear_hidden(x))\n",
        "                  x = self.batch_normalization(x)\n",
        "               y_hat = nn.Softmax(dim=1)(self.linear_out(x))\n",
        "               return y_hat\n",
        "         baseline_model = Baseline(self)\n",
        "         return baseline_model\n",
        "\n",
        " #------------------------------------------------------------------------------#   \n",
        "\n",
        "    def train(self, x_train, y_train):\n",
        "   \n",
        "        for i in range(self.iteration):\n",
        "         \n",
        "              ## Train critic\n",
        "              # Select a random batch of samples\n",
        "              idx = np.random.randint(0, x_train.shape[0], self.batch_size)\n",
        "              x_batch = x_train[idx,:]\n",
        "              y_batch = y_train[idx,:]\n",
        "        for j in range(0, x_train.shape[0], self.batch_size):\n",
        "              # Generate a batch of selection probability\n",
        "              self.actor.eval()\n",
        "              with torch.no_grad():\n",
        "                selection_probability = self.actor(x_batch)                        \n",
        "              # Sampling the features based on the selection_probability\n",
        "              selection = bernoulli_sampling(selection_probability)  \n",
        "         #for j in range(0, x_train.shape[0], self.batch_size): TODO\n",
        "              # Critic loss\n",
        "              self.critic.train()\n",
        "              self.critic.optimizer.zero_grad()\n",
        "              #Forward pass\n",
        "              self.critic.output = self.critic(x_batch, selection)\n",
        "              self.critic.loss_value = self.critic.loss(self.critic.output, y_batch)                        \n",
        "              # Backward pass\n",
        "              self.critic.loss_value.backward()\n",
        "              # Update weights\n",
        "              self.critic.optimizer.step()\n",
        "\n",
        "\n",
        "              # Baseline output\n",
        "              if self.model_type == 'invase':   \n",
        "                  self.baseline.train()\n",
        "                  # Baseline loss\n",
        "                  self.baseline.optimizer.zero_grad()\n",
        "                  # Forward pass\n",
        "                  self.baseline.output = self.baseline(x_batch)\n",
        "                  self.baseline.loss_value = self.baseline.loss(self.baseline.output, y_batch)                        \n",
        "                  # Backward pass\n",
        "                  self.baseline.loss_value.backward()\n",
        "                  # Update weights\n",
        "                  self.baseline.optimizer.step()\n",
        "\n",
        "              ## Train actor\n",
        "              # Use multiple things as the y_true: \n",
        "              # - selection, critic_out, baseline_out, and ground truth (y_batch)\n",
        "              if self.model_type == 'invase':\n",
        "    \n",
        "                y_batch_final = torch.cat((torch.from_numpy(selection), self.critic.output,\n",
        "                                          self.baseline.output, y_batch), axis = 1)\n",
        "              elif self.model_type == 'invase_minus':\n",
        "                y_batch_final = torch.cat((torch.from_numpy(selection), self.critic.output,\n",
        "                                          y_batch), axis = 1)\n",
        "              # Train the actor\n",
        "              self.actor.train()\n",
        "              self.actor.optimizer.zero_grad()\n",
        "              # Forward pass\n",
        "              self.actor.output = self.actor(x_batch)\n",
        "\n",
        "              self.actor.loss_value = self.actor.loss(y_batch_final.detach(), self.actor.output)                       \n",
        "              # Backward pass\n",
        "              self.actor.loss_value.backward()\n",
        "              # Update weights\n",
        "              self.actor.optimizer.step()\n",
        "                                                          \n",
        "\n",
        "              # Print the progress\n",
        "              if self.model_type == 'invase':\n",
        "                  dialog = 'Iterations: ' + str(i) + \\\n",
        "                          ', critic loss: ' + str(self.critic.loss_value) + \\\n",
        "                          ', baseline loss: ' + str(self.baseline.loss_value) + \\\n",
        "                          ', actor loss: ' + str(self.actor.loss_value)\n",
        "              elif self.model_type == 'invase_minus':\n",
        "                  dialog = 'Iterations: ' + str(i) + \\\n",
        "                          ', critic loss: ' + str(self.critic.loss_value) + \\\n",
        "                          ', actor loss: ' + str(self.actor.loss_value)\n",
        "              if i % 1000 == 0:\n",
        "                  print(dialog)\n",
        "\n",
        "    def importance_score(self, x):\n",
        "        self.actor.eval()\n",
        "        with torch.no_grad():\n",
        "          feature_importance = self.actor(x)    \n",
        "        return np.asarray(feature_importance)\n",
        "\n",
        "    def predict(self, x_test):\n",
        "         \n",
        "        # Generate a batch of selection probability\n",
        "        self.actor.eval()\n",
        "        with torch.no_grad():\n",
        "          selection_probability = self.actor(x_test)            \n",
        "        # Sampling the features based on the selection_probability\n",
        "        selection = bernoulli_sampling(selection_probability)   \n",
        "        # Prediction \n",
        "        self.critic.eval()\n",
        "        with torch.no_grad():\n",
        "           y_hat = self.critic(x_test, selection)          \n",
        "        return np.asarray(y_hat)\n"
      ],
      "metadata": {
        "id": "w5MU2v2fBo3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u7pI_a8bmuTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Main**\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "(1) Data generation\n",
        "\n",
        "(2) Train INVASE or INVASE-\n",
        "\n",
        "(3) Evaluate INVASE on ground truth feature importance and prediction"
      ],
      "metadata": {
        "id": "Cfgru9K1IKUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Paraméterel megadása a modellnek\n",
        "class args:\n",
        "  data_type: str\n",
        "  train_no: int\n",
        "  test_no: int\n",
        "  dim : int\n",
        "  model_type : str\n",
        "  actor_h_dim : int\n",
        "  critic_h_dim :int\n",
        "  n_layer: int\n",
        "  batch_size : int\n",
        "  iteration : int\n",
        "  activation : str\n",
        "  learning_rate : float\n",
        "  lamda : float\n",
        "\n",
        "# Inputs for the main function\n",
        "args.data_type = 'syn1'\n",
        "args.train_no = 10000\n",
        "args.test_no = 10000\n",
        "args.dim = 11\n",
        "args.model_type = 'invase'\n",
        "args.actor_h_dim = 100\n",
        "args.critic_h_dim = 200\n",
        "args.n_layer = 3\n",
        "args.batch_size = 1000\n",
        "args.iteration = 10000\n",
        "args.activation = 'relu'\n",
        "args.learning_rate = 0.0001\n",
        "args.lamda = 0.1\n"
      ],
      "metadata": {
        "id": "9ozHOjy-Z8lU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate dataset\n",
        "x_train, y_train, g_train = generate_dataset (n = args.train_no, \n",
        "                                              dim = args.dim, \n",
        "                                              data_type = args.data_type, \n",
        "                                              seed = 0)\n",
        "\n",
        "x_test, y_test, g_test = generate_dataset (n = args.test_no,\n",
        "                                            dim = args.dim, \n",
        "                                            data_type = args.data_type, \n",
        "                                            seed = 0)\n",
        "\n",
        "model_parameters = {'lamda': args.lamda,\n",
        "                    'actor_h_dim': args.actor_h_dim, \n",
        "                    'critic_h_dim': args.critic_h_dim,\n",
        "                    'n_layer': args.n_layer,\n",
        "                    'batch_size': args.batch_size,\n",
        "                    'iteration': args.iteration, \n",
        "                    'activation': args.activation, \n",
        "                    'learning_rate': args.learning_rate}\n",
        "\n",
        "# Train the model\n",
        "\n",
        "x_train_tensor = torch.from_numpy(x_train).float()\n",
        "y_train_tensor = torch.from_numpy(y_train).float()\n",
        "model = Invase(x_train_tensor, y_train_tensor, args.model_type, model_parameters)\n",
        "\n",
        "model.train(x_train_tensor, y_train_tensor)    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p3mzqGC0bZ_",
        "outputId": "b9b1f423-8eba-4cb7-8fed-d8556ad9b139"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iterations: 0, critic loss: tensor(0.7140, grad_fn=<DivBackward1>), baseline loss: tensor(0.7145, grad_fn=<DivBackward1>), actor loss: tensor(0.0434, grad_fn=<MeanBackward0>)\n",
            "Iterations: 0, critic loss: tensor(0.7068, grad_fn=<DivBackward1>), baseline loss: tensor(0.7026, grad_fn=<DivBackward1>), actor loss: tensor(-0.0505, grad_fn=<MeanBackward0>)\n",
            "Iterations: 0, critic loss: tensor(0.7114, grad_fn=<DivBackward1>), baseline loss: tensor(0.7156, grad_fn=<DivBackward1>), actor loss: tensor(0.1297, grad_fn=<MeanBackward0>)\n",
            "Iterations: 0, critic loss: tensor(0.7107, grad_fn=<DivBackward1>), baseline loss: tensor(0.7032, grad_fn=<DivBackward1>), actor loss: tensor(-0.0960, grad_fn=<MeanBackward0>)\n",
            "Iterations: 0, critic loss: tensor(0.7093, grad_fn=<DivBackward1>), baseline loss: tensor(0.7164, grad_fn=<DivBackward1>), actor loss: tensor(0.1679, grad_fn=<MeanBackward0>)\n",
            "Iterations: 0, critic loss: tensor(0.7062, grad_fn=<DivBackward1>), baseline loss: tensor(0.7006, grad_fn=<DivBackward1>), actor loss: tensor(-0.0235, grad_fn=<MeanBackward0>)\n",
            "Iterations: 0, critic loss: tensor(0.6982, grad_fn=<DivBackward1>), baseline loss: tensor(0.7050, grad_fn=<DivBackward1>), actor loss: tensor(0.1592, grad_fn=<MeanBackward0>)\n",
            "Iterations: 0, critic loss: tensor(0.7090, grad_fn=<DivBackward1>), baseline loss: tensor(0.7051, grad_fn=<DivBackward1>), actor loss: tensor(-0.0351, grad_fn=<MeanBackward0>)\n",
            "Iterations: 0, critic loss: tensor(0.7129, grad_fn=<DivBackward1>), baseline loss: tensor(0.6988, grad_fn=<DivBackward1>), actor loss: tensor(-0.1604, grad_fn=<MeanBackward0>)\n",
            "Iterations: 0, critic loss: tensor(0.7038, grad_fn=<DivBackward1>), baseline loss: tensor(0.6944, grad_fn=<DivBackward1>), actor loss: tensor(-0.1207, grad_fn=<MeanBackward0>)\n",
            "Iterations: 1000, critic loss: tensor(0.6940, grad_fn=<DivBackward1>), baseline loss: tensor(0.3371, grad_fn=<DivBackward1>), actor loss: tensor(-0.3347, grad_fn=<MeanBackward0>)\n",
            "Iterations: 1000, critic loss: tensor(0.6929, grad_fn=<DivBackward1>), baseline loss: tensor(0.3369, grad_fn=<DivBackward1>), actor loss: tensor(-0.4116, grad_fn=<MeanBackward0>)\n",
            "Iterations: 1000, critic loss: tensor(0.6912, grad_fn=<DivBackward1>), baseline loss: tensor(0.3370, grad_fn=<DivBackward1>), actor loss: tensor(-0.4065, grad_fn=<MeanBackward0>)\n",
            "Iterations: 1000, critic loss: tensor(0.6922, grad_fn=<DivBackward1>), baseline loss: tensor(0.3361, grad_fn=<DivBackward1>), actor loss: tensor(-0.2742, grad_fn=<MeanBackward0>)\n",
            "Iterations: 1000, critic loss: tensor(0.6931, grad_fn=<DivBackward1>), baseline loss: tensor(0.3311, grad_fn=<DivBackward1>), actor loss: tensor(-0.3025, grad_fn=<MeanBackward0>)\n",
            "Iterations: 1000, critic loss: tensor(0.6933, grad_fn=<DivBackward1>), baseline loss: tensor(0.3393, grad_fn=<DivBackward1>), actor loss: tensor(-0.4244, grad_fn=<MeanBackward0>)\n",
            "Iterations: 1000, critic loss: tensor(0.6910, grad_fn=<DivBackward1>), baseline loss: tensor(0.3390, grad_fn=<DivBackward1>), actor loss: tensor(-0.3108, grad_fn=<MeanBackward0>)\n",
            "Iterations: 1000, critic loss: tensor(0.6955, grad_fn=<DivBackward1>), baseline loss: tensor(0.3355, grad_fn=<DivBackward1>), actor loss: tensor(-0.3674, grad_fn=<MeanBackward0>)\n",
            "Iterations: 1000, critic loss: tensor(0.6947, grad_fn=<DivBackward1>), baseline loss: tensor(0.3408, grad_fn=<DivBackward1>), actor loss: tensor(-0.2129, grad_fn=<MeanBackward0>)\n",
            "Iterations: 1000, critic loss: tensor(0.6910, grad_fn=<DivBackward1>), baseline loss: tensor(0.3354, grad_fn=<DivBackward1>), actor loss: tensor(-0.2649, grad_fn=<MeanBackward0>)\n",
            "Iterations: 2000, critic loss: tensor(0.6897, grad_fn=<DivBackward1>), baseline loss: tensor(0.3301, grad_fn=<DivBackward1>), actor loss: tensor(-0.0961, grad_fn=<MeanBackward0>)\n",
            "Iterations: 2000, critic loss: tensor(0.6899, grad_fn=<DivBackward1>), baseline loss: tensor(0.3262, grad_fn=<DivBackward1>), actor loss: tensor(-0.1070, grad_fn=<MeanBackward0>)\n",
            "Iterations: 2000, critic loss: tensor(0.6905, grad_fn=<DivBackward1>), baseline loss: tensor(0.3273, grad_fn=<DivBackward1>), actor loss: tensor(-0.0923, grad_fn=<MeanBackward0>)\n",
            "Iterations: 2000, critic loss: tensor(0.6895, grad_fn=<DivBackward1>), baseline loss: tensor(0.3300, grad_fn=<DivBackward1>), actor loss: tensor(-0.1069, grad_fn=<MeanBackward0>)\n",
            "Iterations: 2000, critic loss: tensor(0.6888, grad_fn=<DivBackward1>), baseline loss: tensor(0.3280, grad_fn=<DivBackward1>), actor loss: tensor(-0.0691, grad_fn=<MeanBackward0>)\n",
            "Iterations: 2000, critic loss: tensor(0.6862, grad_fn=<DivBackward1>), baseline loss: tensor(0.3283, grad_fn=<DivBackward1>), actor loss: tensor(-0.0485, grad_fn=<MeanBackward0>)\n",
            "Iterations: 2000, critic loss: tensor(0.6899, grad_fn=<DivBackward1>), baseline loss: tensor(0.3271, grad_fn=<DivBackward1>), actor loss: tensor(-0.1097, grad_fn=<MeanBackward0>)\n",
            "Iterations: 2000, critic loss: tensor(0.6914, grad_fn=<DivBackward1>), baseline loss: tensor(0.3262, grad_fn=<DivBackward1>), actor loss: tensor(-0.0872, grad_fn=<MeanBackward0>)\n",
            "Iterations: 2000, critic loss: tensor(0.6916, grad_fn=<DivBackward1>), baseline loss: tensor(0.3320, grad_fn=<DivBackward1>), actor loss: tensor(-0.1240, grad_fn=<MeanBackward0>)\n",
            "Iterations: 2000, critic loss: tensor(0.6881, grad_fn=<DivBackward1>), baseline loss: tensor(0.3251, grad_fn=<DivBackward1>), actor loss: tensor(-0.1102, grad_fn=<MeanBackward0>)\n",
            "Iterations: 3000, critic loss: tensor(0.6856, grad_fn=<DivBackward1>), baseline loss: tensor(0.3248, grad_fn=<DivBackward1>), actor loss: tensor(-0.0387, grad_fn=<MeanBackward0>)\n",
            "Iterations: 3000, critic loss: tensor(0.6867, grad_fn=<DivBackward1>), baseline loss: tensor(0.3220, grad_fn=<DivBackward1>), actor loss: tensor(-0.0056, grad_fn=<MeanBackward0>)\n",
            "Iterations: 3000, critic loss: tensor(0.6865, grad_fn=<DivBackward1>), baseline loss: tensor(0.3239, grad_fn=<DivBackward1>), actor loss: tensor(-0.0649, grad_fn=<MeanBackward0>)\n",
            "Iterations: 3000, critic loss: tensor(0.6855, grad_fn=<DivBackward1>), baseline loss: tensor(0.3200, grad_fn=<DivBackward1>), actor loss: tensor(-0.0794, grad_fn=<MeanBackward0>)\n",
            "Iterations: 3000, critic loss: tensor(0.6869, grad_fn=<DivBackward1>), baseline loss: tensor(0.3240, grad_fn=<DivBackward1>), actor loss: tensor(-0.0577, grad_fn=<MeanBackward0>)\n",
            "Iterations: 3000, critic loss: tensor(0.6826, grad_fn=<DivBackward1>), baseline loss: tensor(0.3230, grad_fn=<DivBackward1>), actor loss: tensor(-0.0340, grad_fn=<MeanBackward0>)\n",
            "Iterations: 3000, critic loss: tensor(0.6871, grad_fn=<DivBackward1>), baseline loss: tensor(0.3258, grad_fn=<DivBackward1>), actor loss: tensor(-0.0106, grad_fn=<MeanBackward0>)\n",
            "Iterations: 3000, critic loss: tensor(0.6911, grad_fn=<DivBackward1>), baseline loss: tensor(0.3229, grad_fn=<DivBackward1>), actor loss: tensor(-0.1552, grad_fn=<MeanBackward0>)\n",
            "Iterations: 3000, critic loss: tensor(0.6866, grad_fn=<DivBackward1>), baseline loss: tensor(0.3230, grad_fn=<DivBackward1>), actor loss: tensor(-0.0435, grad_fn=<MeanBackward0>)\n",
            "Iterations: 3000, critic loss: tensor(0.6870, grad_fn=<DivBackward1>), baseline loss: tensor(0.3199, grad_fn=<DivBackward1>), actor loss: tensor(-0.1403, grad_fn=<MeanBackward0>)\n",
            "Iterations: 4000, critic loss: tensor(0.6838, grad_fn=<DivBackward1>), baseline loss: tensor(0.3244, grad_fn=<DivBackward1>), actor loss: tensor(-0.0313, grad_fn=<MeanBackward0>)\n",
            "Iterations: 4000, critic loss: tensor(0.6842, grad_fn=<DivBackward1>), baseline loss: tensor(0.3224, grad_fn=<DivBackward1>), actor loss: tensor(0.0016, grad_fn=<MeanBackward0>)\n",
            "Iterations: 4000, critic loss: tensor(0.6857, grad_fn=<DivBackward1>), baseline loss: tensor(0.3244, grad_fn=<DivBackward1>), actor loss: tensor(-0.0226, grad_fn=<MeanBackward0>)\n",
            "Iterations: 4000, critic loss: tensor(0.6847, grad_fn=<DivBackward1>), baseline loss: tensor(0.3203, grad_fn=<DivBackward1>), actor loss: tensor(-0.0252, grad_fn=<MeanBackward0>)\n",
            "Iterations: 4000, critic loss: tensor(0.6896, grad_fn=<DivBackward1>), baseline loss: tensor(0.3195, grad_fn=<DivBackward1>), actor loss: tensor(-0.1843, grad_fn=<MeanBackward0>)\n",
            "Iterations: 4000, critic loss: tensor(0.6867, grad_fn=<DivBackward1>), baseline loss: tensor(0.3225, grad_fn=<DivBackward1>), actor loss: tensor(-0.1222, grad_fn=<MeanBackward0>)\n",
            "Iterations: 4000, critic loss: tensor(0.6875, grad_fn=<DivBackward1>), baseline loss: tensor(0.3253, grad_fn=<DivBackward1>), actor loss: tensor(-0.0322, grad_fn=<MeanBackward0>)\n",
            "Iterations: 4000, critic loss: tensor(0.6868, grad_fn=<DivBackward1>), baseline loss: tensor(0.3224, grad_fn=<DivBackward1>), actor loss: tensor(-0.0256, grad_fn=<MeanBackward0>)\n",
            "Iterations: 4000, critic loss: tensor(0.6859, grad_fn=<DivBackward1>), baseline loss: tensor(0.3215, grad_fn=<DivBackward1>), actor loss: tensor(-0.0925, grad_fn=<MeanBackward0>)\n",
            "Iterations: 4000, critic loss: tensor(0.6824, grad_fn=<DivBackward1>), baseline loss: tensor(0.3203, grad_fn=<DivBackward1>), actor loss: tensor(-0.0080, grad_fn=<MeanBackward0>)\n",
            "Iterations: 5000, critic loss: tensor(0.6853, grad_fn=<DivBackward1>), baseline loss: tensor(0.3233, grad_fn=<DivBackward1>), actor loss: tensor(-0.0341, grad_fn=<MeanBackward0>)\n",
            "Iterations: 5000, critic loss: tensor(0.6833, grad_fn=<DivBackward1>), baseline loss: tensor(0.3214, grad_fn=<DivBackward1>), actor loss: tensor(0.0677, grad_fn=<MeanBackward0>)\n",
            "Iterations: 5000, critic loss: tensor(0.6825, grad_fn=<DivBackward1>), baseline loss: tensor(0.3234, grad_fn=<DivBackward1>), actor loss: tensor(-0.0421, grad_fn=<MeanBackward0>)\n",
            "Iterations: 5000, critic loss: tensor(0.6839, grad_fn=<DivBackward1>), baseline loss: tensor(0.3203, grad_fn=<DivBackward1>), actor loss: tensor(-0.0088, grad_fn=<MeanBackward0>)\n",
            "Iterations: 5000, critic loss: tensor(0.6873, grad_fn=<DivBackward1>), baseline loss: tensor(0.3194, grad_fn=<DivBackward1>), actor loss: tensor(-0.1187, grad_fn=<MeanBackward0>)\n",
            "Iterations: 5000, critic loss: tensor(0.6849, grad_fn=<DivBackward1>), baseline loss: tensor(0.3224, grad_fn=<DivBackward1>), actor loss: tensor(-0.0514, grad_fn=<MeanBackward0>)\n",
            "Iterations: 5000, critic loss: tensor(0.6862, grad_fn=<DivBackward1>), baseline loss: tensor(0.3252, grad_fn=<DivBackward1>), actor loss: tensor(-0.0606, grad_fn=<MeanBackward0>)\n",
            "Iterations: 5000, critic loss: tensor(0.6885, grad_fn=<DivBackward1>), baseline loss: tensor(0.3214, grad_fn=<DivBackward1>), actor loss: tensor(-0.0896, grad_fn=<MeanBackward0>)\n",
            "Iterations: 5000, critic loss: tensor(0.6857, grad_fn=<DivBackward1>), baseline loss: tensor(0.3214, grad_fn=<DivBackward1>), actor loss: tensor(-0.0527, grad_fn=<MeanBackward0>)\n",
            "Iterations: 5000, critic loss: tensor(0.6843, grad_fn=<DivBackward1>), baseline loss: tensor(0.3202, grad_fn=<DivBackward1>), actor loss: tensor(-0.0417, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Evaluation\n",
        "# Compute importance score\n",
        "x_test_tensor = torch.from_numpy(x_test).float()\n",
        "\n",
        "g_hat = model.importance_score(x_test_tensor)\n",
        "importance_score = 1.*(g_hat > 0.5)"
      ],
      "metadata": {
        "id": "6vaxxYpovqTA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the performance of feature importance\n",
        "mean_tpr, std_tpr, mean_fdr, std_fdr = \\\n",
        "feature_performance_metric(g_test, importance_score)\n",
        "  \n",
        "# Print the performance of feature importance    \n",
        "print('TPR mean: ' + str(np.round(mean_tpr,1)) + '\\%, ' + \\\n",
        "      'TPR std: ' + str(np.round(std_tpr,1)) + '\\%, ')\n",
        "print('FDR mean: ' + str(np.round(mean_fdr,1)) + '\\%, ' + \\\n",
        "      'FDR std: ' + str(np.round(std_fdr,1)) + '\\%, ')"
      ],
      "metadata": {
        "id": "NDtDKc45IcBW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict labels\n",
        "y_hat = model.predict(x_test_tensor)\n",
        "  \n",
        "# Evaluate the performance of feature importance\n",
        "auc, apr, acc = prediction_performance_metric(y_test, y_hat)\n",
        "  \n",
        "# Print the performance of feature importance    \n",
        "print('AUC: ' + str(np.round(auc, 3)) + \\\n",
        "      ', APR: ' + str(np.round(apr, 3)) + \\\n",
        "      ', ACC: ' + str(np.round(acc, 3)))\n",
        "\n",
        "performance = {'mean_tpr': mean_tpr, 'std_tpr': std_tpr,\n",
        "                'mean_fdr': mean_fdr, 'std_fdr': std_fdr,\n",
        "                'auc': auc, 'apr': apr, 'acc': acc}\n",
        "print(performance)"
      ],
      "metadata": {
        "id": "jSSbOZhJvtqh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}